{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa3dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pickleshare=0.7.5\n",
    "!pip install numpy=1.20.2\n",
    "!pip install tqdm=4.62.2\n",
    "!pip install torch=1.8.1\n",
    "!pip install tensorboardx=2.4\n",
    "!pip install scipy=1.7.0\n",
    "!pip install lripy=0.0.2\n",
    "!pip install torchvision=0.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053bcd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://modelarts-cnnorth1-market-dataset.obs.cn-north-1.myhuaweicloud.com/example-apps/FedVal/code.zip\n",
    "!unzip -qo code.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6857b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "from options import args_parser\n",
    "from update import LocalUpdate, test_inference\n",
    "from models import MLP, CNNCifar2, CNNMnist, CNNFashion_Mnist, VGG16, Logistic\n",
    "from utils import get_dataset, average_weights, exp_details\n",
    "from itertools import chain, combinations\n",
    "import operator as op\n",
    "from functools import reduce\n",
    "import sys\n",
    "from scipy.sparse import csr_matrix\n",
    "import pdb\n",
    "from lripy import drcomplete\n",
    "from numpy import linalg as LA\n",
    "from torchvision import datasets, transforms\n",
    "from sampling import mnist_iid, mnist_noniid, mnist_noniid_unequal\n",
    "from sampling import cifar_iid, cifar_noniid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2fe9478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for generating the power set\n",
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    l = chain.from_iterable(combinations(s, r) for r in range(1,len(s)+1))\n",
    "    return {tuple(sorted(tmp)):i for i,tmp in enumerate(l)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cc51ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for computing the combinatorial number\n",
    "def ncr(n, r):\n",
    "    r = min(r, n-r)\n",
    "    numer = reduce(op.mul, range(n, n-r, -1), 1)\n",
    "    denom = reduce(op.mul, range(1, r+1), 1)\n",
    "    return numer // denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f278d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roundly utility function\n",
    "def utility(args, previous_loss, fake_model, weights, test_dataset):\n",
    "    fake_weights = average_weights(weights)\n",
    "    fake_model.load_state_dict(fake_weights)\n",
    "    _, fake_test_loss = test_inference(args, fake_model, test_dataset)\n",
    "    return previous_loss - fake_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05d4c2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute all utilities for selected clients\n",
    "def compute_utilities(args, previous_loss, fake_model, all_subsets, local_weights, test_dataset, idxs_users):\n",
    "    utilities = np.zeros( len(all_subsets) )\n",
    "    utilities_dict = {}\n",
    "    for i, indices in enumerate(tqdm(all_subsets.keys())):\n",
    "        weights = [local_weights[j] for j in indices]\n",
    "        u = utility(args, previous_loss, fake_model, weights, test_dataset)\n",
    "        utilities[i] = u\n",
    "        utilities_dict[indices] = u\n",
    "    return utilities, utilities_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "880cd375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline\n",
    "def compute_shapley_value_baseline(args, utilities_dict, idxs_users):\n",
    "    N = len(idxs_users)\n",
    "    roundly_valuation_baseline = np.zeros(args.num_users)\n",
    "    for i in range( len(idxs_users) ):\n",
    "        tmp_indices = list(idxs_users)\n",
    "        current_i = tmp_indices.pop(i)\n",
    "        subpowerset = powerset(tmp_indices)\n",
    "        val = 0\n",
    "        for s in subpowerset.keys():\n",
    "            si=tuple(sorted(list(s)+[current_i]))\n",
    "            val += (utilities_dict[si]-utilities_dict[s]) / ncr(N - 1, len(s))\n",
    "        roundly_valuation_baseline[current_i] = val / N\n",
    "    return roundly_valuation_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ab34a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth\n",
    "def compute_shapley_value_groundtruth(args, utilities_dict):\n",
    "    N = args.num_users\n",
    "    roundly_valuation_groundtruth = np.zeros(N)\n",
    "    for i in range( N ):\n",
    "        tmp_indices = list(range(N))\n",
    "        current_i = tmp_indices.pop(i)\n",
    "        subpowerset = powerset(tmp_indices)\n",
    "        val = 0\n",
    "        for s in subpowerset.keys():\n",
    "            si=tuple(sorted(list(s)+[current_i]))\n",
    "            val += (utilities_dict[si]-utilities_dict[s]) / ncr(N - 1, len(s))\n",
    "        roundly_valuation_groundtruth[current_i] = val / N\n",
    "    return roundly_valuation_groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81d3e180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask in each round\n",
    "def roundly_mask(idxs_users, all_subsets):\n",
    "    mask_vec = np.zeros( len(all_subsets) )\n",
    "    subpowerset = powerset(idxs_users)\n",
    "    for s in subpowerset.keys():\n",
    "        i = all_subsets[s]\n",
    "        mask_vec[i] = 1\n",
    "    return mask_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c92d0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our approach\n",
    "def compute_shapley_value_from_matrix(args, utility_matrix, all_subsets):\n",
    "    T = args.epochs\n",
    "    N = args.num_users\n",
    "    valuation_completed = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        sublist = list(range(N))\n",
    "        sublist.pop(i)\n",
    "        subpowerset = powerset(sublist)\n",
    "        for s in subpowerset.keys():\n",
    "            # id1 = all_subsets.index(s)\n",
    "            id1 = all_subsets[s]\n",
    "            id2 = all_subsets[tuple(sorted(list(s)+[i]))]\n",
    "            for t in range(T):\n",
    "                v1 = utility_matrix[t, id1]\n",
    "                v2 = utility_matrix[t, id2]\n",
    "                val = (v2 - v1) / ncr(N - 1, len(s))\n",
    "                valuation_completed[i] += val\n",
    "        valuation_completed[i] /= N\n",
    "    return valuation_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2aeac991",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    def __init__(self):\n",
    "        self.epochs = 10\n",
    "        self.num_users = 10\n",
    "        self.frac = 0.5\n",
    "        self.local_ep = 1\n",
    "        self.local_bs = 50\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.0\n",
    "        self.optimizer = 'sgd'\n",
    "        self.gpu = 1\n",
    "        self.verbose = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eefef047",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a12e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_dir = '../data/mnist/'\n",
    "apply_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = datasets.MNIST(data_dir, train=True, download=True,\n",
    "                                       transform=apply_transform)\n",
    "test_dataset = datasets.MNIST(data_dir, train=False, download=True,\n",
    "                                      transform=apply_transform)\n",
    "test_dataset.data = test_dataset.data[:1000]\n",
    "test_dataset.targets = test_dataset.targets[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc88fe9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhenan/miniconda3/envs/fedval/lib/python3.7/site-packages/torchvision/datasets/mnist.py:54: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "# split data in a non-iid or iid fashion\n",
    "# client 0 and 9 have same data\n",
    "user_groups = mnist_noniid(train_dataset, args.num_users)\n",
    "# user_groups = mnist_iid(train_dataset, args.num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "089a675c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layer_input): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (layer_hidden): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (logsoftmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# build MLP model\n",
    "img_size = train_dataset[0][0].shape\n",
    "len_in = 1\n",
    "for x in img_size:\n",
    "    len_in *= x\n",
    "#     global_model=Logistic(in_dim=len_in, out_dim=10)\n",
    "global_model = MLP(dim_in=len_in, dim_hidden=64,\n",
    "                       dim_out=10)\n",
    "device = 'cuda'\n",
    "global_model.to(device)\n",
    "global_model.train()\n",
    "global_weights = global_model.state_dict()\n",
    "print(global_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51e43b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "train_loss, train_accuracy = [], []\n",
    "val_acc_list, net_list = [], []\n",
    "cv_loss, cv_acc = [], []\n",
    "print_every = 1\n",
    "val_loss_pre, counter = 0, 0\n",
    "valuation_baseline = np.zeros(args.num_users)\n",
    "valuation_groundtruth = np.zeros(args.num_users)\n",
    "all_subsets = powerset(range(args.num_users))\n",
    "utility_matrix = np.zeros( (args.epochs, len(all_subsets.keys())) )\n",
    "mask = np.zeros( (args.epochs, len(all_subsets.keys())) )\n",
    "previous_loss = 0.0\n",
    "fake_model=copy.deepcopy(global_model)\n",
    "# define paths\n",
    "path_project = os.path.abspath('..')\n",
    "logger = SummaryWriter('../logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "581f4cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " | Global Training Round : 1 |\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhenan/Federated-Learning-PyTorch/src/update.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(image), torch.tensor(label)\n",
      "100%|█████████████████████████████████████████████████████████| 1023/1023 [01:44<00:00,  9.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Avg Training Stats after 1 global rounds:\n",
      "Training Loss : 1.6753390914201738\n",
      "Train Accuracy: 1.67% \n",
      "\n",
      "\n",
      " | Global Training Round : 2 |\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 1023/1023 [01:45<00:00,  9.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Avg Training Stats after 2 global rounds:\n",
      "Training Loss : 1.5184518268704414\n",
      "Train Accuracy: 96.67% \n",
      "\n",
      "\n",
      " | Global Training Round : 3 |\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 1023/1023 [01:47<00:00,  9.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Avg Training Stats after 3 global rounds:\n",
      "Training Loss : 1.4072828886906306\n",
      "Train Accuracy: 48.33% \n",
      "\n",
      "\n",
      " | Global Training Round : 4 |\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 1023/1023 [01:45<00:00,  9.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Avg Training Stats after 4 global rounds:\n",
      "Training Loss : 1.3167072431743145\n",
      "Train Accuracy: 95.00% \n",
      "\n",
      "\n",
      " | Global Training Round : 5 |\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 1023/1023 [01:46<00:00,  9.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Avg Training Stats after 5 global rounds:\n",
      "Training Loss : 1.2394589634537696\n",
      "Train Accuracy: 81.67% \n",
      "\n",
      "\n",
      " | Global Training Round : 6 |\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 1023/1023 [01:45<00:00,  9.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Avg Training Stats after 6 global rounds:\n",
      "Training Loss : 1.1754381977021693\n",
      "Train Accuracy: 85.00% \n",
      "\n",
      "\n",
      " | Global Training Round : 7 |\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 1023/1023 [01:44<00:00,  9.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Avg Training Stats after 7 global rounds:\n",
      "Training Loss : 1.1244683280161447\n",
      "Train Accuracy: 31.67% \n",
      "\n",
      "\n",
      " | Global Training Round : 8 |\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 1023/1023 [01:45<00:00,  9.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Avg Training Stats after 8 global rounds:\n",
      "Training Loss : 1.077837294526398\n",
      "Train Accuracy: 80.00% \n",
      "\n",
      "\n",
      " | Global Training Round : 9 |\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 1023/1023 [01:44<00:00,  9.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Avg Training Stats after 9 global rounds:\n",
      "Training Loss : 1.0380750592384074\n",
      "Train Accuracy: 88.33% \n",
      "\n",
      "\n",
      " | Global Training Round : 10 |\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 1023/1023 [01:44<00:00,  9.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Avg Training Stats after 10 global rounds:\n",
      "Training Loss : 1.0063412492573263\n",
      "Train Accuracy: 91.67% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# main loop of training\n",
    "for epoch in range(args.epochs):\n",
    "    local_weights, local_losses = [], []\n",
    "    print(f'\\n | Global Training Round : {epoch+1} |\\n')\n",
    "    global_model.train()\n",
    "    \n",
    "    # select participants\n",
    "    m = max(int(args.frac * args.num_users), 1)\n",
    "    if epoch == 0:\n",
    "        idxs_users = list(range(args.num_users))\n",
    "    else:\n",
    "        idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "        \n",
    "    # train local models\n",
    "    for idx in range(args.num_users):\n",
    "        local_model = LocalUpdate(args=args, dataset=train_dataset,\n",
    "                                    idxs=user_groups[idx], logger=logger, client_idx=idx, is_noisy=False)\n",
    "        if idx==args.num_users-1:\n",
    "            w = copy.deepcopy(local_weights[0])\n",
    "            loss = copy.deepcopy(local_losses[0])\n",
    "        else:\n",
    "            w, loss = local_model.update_weights(model=copy.deepcopy(global_model), global_round=epoch)\n",
    "        local_weights.append(copy.deepcopy(w))\n",
    "        local_losses.append(copy.deepcopy(loss))\n",
    "        \n",
    "    # compute shapley values\n",
    "    utilities, utilities_dict = compute_utilities(args, previous_loss, fake_model, \n",
    "                                                  all_subsets, local_weights, test_dataset, idxs_users)\n",
    "    utility_matrix[epoch, :] = utilities\n",
    "    mask_epoch = roundly_mask(idxs_users, all_subsets)\n",
    "    mask[epoch, :] = mask_epoch\n",
    "    valuation_baseline_epoch = compute_shapley_value_baseline(args, utilities_dict, idxs_users)\n",
    "    valuation_baseline += valuation_baseline_epoch\n",
    "    valuation_groundtruth_epoch = compute_shapley_value_groundtruth(args, utilities_dict)\n",
    "    valuation_groundtruth += valuation_groundtruth_epoch\n",
    "    \n",
    "    # update global weights\n",
    "    local_weights_selected = [local_weights[i] for i in list(idxs_users)]\n",
    "    global_weights = average_weights(local_weights_selected)\n",
    "    global_model.load_state_dict(global_weights)\n",
    "    loss_avg = sum(local_losses) / len(local_losses)\n",
    "    train_loss.append(loss_avg)\n",
    "    \n",
    "    # Calculate avg training accuracy over all users at every epoch\n",
    "    list_acc, list_loss = [], []\n",
    "    global_model.eval()\n",
    "    for c in range(args.num_users):\n",
    "        local_model = LocalUpdate(args=args, dataset=train_dataset,\n",
    "                                  idxs=user_groups[idx], logger=logger, client_idx=c, is_noisy=False)\n",
    "        acc, loss = local_model.inference(model=global_model)\n",
    "        list_acc.append(acc)\n",
    "        list_loss.append(loss)\n",
    "    train_accuracy.append(sum(list_acc)/len(list_acc))\n",
    "    \n",
    "    # test loss \n",
    "    test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
    "    previous_loss = test_loss\n",
    "    \n",
    "    # print training and test losses after every 'i' rounds\n",
    "    if (epoch+1) % print_every == 0:\n",
    "        print(f' \\nAvg Training Stats after {epoch+1} global rounds:')\n",
    "        print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
    "        print('Train Accuracy: {:.2f}% \\n'.format(100*train_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83402427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error between X_281 and Y_281 <= 0.1\n",
      "Error between X_511 and Y_511 <= 0.01\n",
      "Error between X_1211 and Y_1211 <= 0.001\n",
      "Error between X_2555 and Y_2555 <= 0.0001\n",
      "Error between X_3899 and Y_3899 <= 1e-05\n"
     ]
    }
   ],
   "source": [
    "# complete matrix\n",
    "mask = mask.astype(int)\n",
    "U = csr_matrix(np.multiply(utility_matrix, mask))\n",
    "utility_matrix_complete = drcomplete(utility_matrix, mask, 3, 2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "009d8c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute ComFedSV\n",
    "valuation_completed = compute_shapley_value_from_matrix(args, utility_matrix_complete, all_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64a67888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "sb = sum(valuation_baseline) \n",
    "valuation_baseline = [v/sb for v in valuation_baseline]\n",
    "sg = sum(valuation_groundtruth) \n",
    "valuation_groundtruth = [v/sg for v in valuation_groundtruth]\n",
    "sc = sum(valuation_completed) \n",
    "valuation_completed = [v/sc for v in valuation_completed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c74f868b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relative diff(FedSV) =  0.09489009013302833\n",
      "relative diff(ComFedSV) =  5.745064990122422e-05\n",
      "relative diff(Ground-truth) =  2.821971767457566e-08\n"
     ]
    }
   ],
   "source": [
    "# print the valuation difference between client 0 and client 9 for all three metrics\n",
    "print(\"relative diff(FedSV) = \", \n",
    "      abs(valuation_baseline[0]-valuation_baseline[9])/min(valuation_baseline[0],valuation_baseline[9]))\n",
    "print(\"relative diff(ComFedSV) = \", \n",
    "      abs(valuation_completed[0]-valuation_completed[9])/min(valuation_completed[0],valuation_completed[9]))\n",
    "print(\"relative diff(Ground-truth) = \", \n",
    "      abs(valuation_groundtruth[0]-valuation_groundtruth[9])/min(valuation_groundtruth[0],valuation_groundtruth[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9281ec04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e71b24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedval",
   "language": "python",
   "name": "fedval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
